Assumptions
* We assume the word is not trimmed at every read.  Full word is captured.
* CHUNK_SIZE must be less than long int (compress_zero)
* We assume CHUNK_SIZE will usually be large - at least equivalent to number of CPUs on the card.
	* In such a case, we assume that h_num_free will very rarely be CHUNK_SIZE.  Code to check this is removed.

following are done:
create_update_hash_mem - for the global str variable in lib_func.cu
K_histo_copy_to_hash_mem
K_histo_words_to_token_first time: As we copy data, we need to balance across multiple histo_chunks...  We are not doing this.
empty_first_token_into_histo COMPLETELY empties the first_token.  It goes to zero.

if val in histo
	put in overflow
	check the copying from wd to histo is done histo by histo
endif

We don't handle the situation of a word crossing multiple d_buf

Initialized_global_structs - check where called.  The first initialization might overwrite mem.

load chunk by chunk
cuda_convert_to_words - loop for each chunk.
Don't read more data than 40% of all chunks
do we need ab -> h_tot_hist


High level:
 Token on card and move histo to cpu
 sort chunk by chunk
 balance chunk by chunk
 delete first_token, holding_data, d_data_loc, to_stor
 keep htop_list in CPU
some code is still TBD for sorting.  Do it as you debug the code.
stop-words: logic for filtering out words - sort, merge, filter in one shot
WE HAVE A PROBLEM IN SORT_CROSS
stop-words - create
	read words from file, convert to mhash
	copy to histo & mv back to host.
	stop-words.out on host (with mhash)

apply stop-words - histogram
	to histo-list - apply stop-words after
stop-words: create separate sorted sw list.
apply stop-words to histo_list

K_update_top_list to-Modify:
 collect all the top_list data after sorting
 get to host and store there.

The following is old - design has changed:
Balance across histo:
copy from overload_token to main till buffer full and then balance across histo. Ensure you update num_free.
Copy to front of main buffer as usually there will not be any free buffers in the middle or the end - sort ensures that.
ensure you update free_buf as you insert & merge.
first balance & then sort and update top.
h_wd- where is it updated
mv'd h_str to ab - modify all *********
 alloc, de_alloc
 usage
when is done marked as false?

Token on card and move histo to cpu
mv data to d_data
review design:
from stream to token - done
sort token - done
balance token as is
get token out - done
print token
compile

 Sort:
 	while (not_sorted)
 		while (not_sorted)
 			sort odd
 			sort even
 		endwhile
 		update htl
 		sort htl and chunks
 		sort across adjoining chunks
 	end while
 	balance all chunks (just insert buffers)
 	update htl

check where all histo_initialize_global_structs is called.  If called multiple times, it can cause a problem.

Can you reduce the number of strn_cpy.  Can we keep the string in one place and swap the address?

garbage clearance

defect: in stop-words - delete all token and tfs except one.  Zero that one out and link them together.

defect: in update_h_hist - when copying tfs to host, cp toen and update the token links

do we need st_words.loc?

tt -> tok_modified = false;
tt -> stop_word_applied = false;

perf/enhance: change tfs only pointer here.  mv mhash into token
Do we need mhash in tfs?

Two errors:
the loop in cuda_convert_to_words is larger than the number of words.
print histo_data is printing wrongly.
exit (1) at cuda_convert_to_words

set_first_last_hash_in_dev
	create_expand_hash_buffer (tot_hist)
	for each histo-chunk
		last_hash = token (end)
		if (prev == 0 && curr) first_hash = curr;
	endfor
 what if curr is at 0 loc
 what if there are no hashes (empty)
 how do we walk down the list of hashes as we update them.
 create dev_list in ab
 copy dev_list to tmp_list in host.
 update each h_hist from tmp_list

 milib-sort-merge
	sort internal
	get-all-hash-back
	sort chunks
	for each two chunks - cross-sort
		if last hash of 1st < 1st hash of 2nd goto next
		else cross-sort
			for whole chunk of next (x)
				if each x > yi
					go prev y till x <=y
					either merge or swap
			endfor
		end cross-sort
	endfor

sort_merge_histo_cross
	create scratchpad
	for every hist chunk = c, next = n
		set d_scr_free
		zero out scratchpad
		cp c to head of scratchpad
		if c.last_mhash > n.first_mhash
			if c.last_mhash >= n.last_mhash
				// merge the two
				copy 2nd chunk to scr
			else	// merge part
				cp part of 2nd chunk to scr
			endif
			sort, merge scr
			balance back to c & n
		else if c.last_mhash == n.first_mhash
			merge c.last & n.first
		endif
	endfor

Balance from Scratchpad to histo
	get value of d_scr_free
	if (2 * CHUNK_SIZE - d_scr_free <= CHUNK_SIZE * 60%)
		copy scratch to curr
		push right
		del (nxt)
		update d_curr_free
		tot_hist--
	else
		if (next_free) - here only subst was done. Ensure you zero out data when copy into scratchpad
			ratio = curr_free / next free
		else
			ratio = 1/2
		endif
		cp ratio * CHUNK_SIZE to curr
		cp (1 - ratio) * CHUNK_SIZE to nxt (prepend this data)
		push right for both
		update curr_free, nxt_free
	endif

create new histo only during reading new data

I'm doing first & last mhash twice - once in update_h_hist and then in generate_first_last_hash.  Decide what I want to do.

delete_histo_chunks is commented out will need it
validated: histo_swap - is not used but we are doing a swap during a sort
check not too many histo_idx created.

K_histo_cpB2End_from_to
outside
	get start point - scan till not zero.
	calculate end point
in device (from start point to end point)
	gTh += start point
	if (gTh > end-point) return
	mv token from scratech to target
	dec (target_free)
	inc (scratch_free)
see if cpB2Beg_from_to and cpB2End_from_to can be merged...
Should we replace K_histo_cp_from_to with cudaMemCpy

while doing memcpy to and from scratchpad - add & dec free_num
free stuff properly. Are we freeing up all histo properly?

are we updating ab -> tot_hist properly - no
TBD: increase ab-> tot_hist (in cuda_convert_to_words) as we add new words and shrink ab->tot_hist as we merge words...

sort the dim3 issue out across the code

Math processing:
h_math - allocate & free

mean and average are the same: tot/tot_nos
median - if odd nos then val of middle one
		else avg of val of lt & rt
st_dev (variance)
	calculate average
	for each num
		n = (num - avg) ** 2
	m = avg of all n
	st_dev = sqrt (m)

math_histo
	(host) for each chunk calculate tot_wds
	store
	(dev) reduce sig (num) to 1 total
	fetch sig (num) total to host
	(host) avg = total / tot_wds
		new thread - calculate median, store
	(dev) cp avg to dev
	for each num var = (num - avg) ** 2
	reduce var to 1 num, cp to host
	(host)
	avg_var = sig var / tot_wds
	std_dev = sqrt (avg_var)
	print
Working here:
update h_num_free and eliminate all the copies of d_num_free to host.
scratchpad needs to be handled properly - not done yet.


do we really need h_read_buf - it can be static? - yes we do.  It will be pinned memory and will need to be freed explicitly.
apply stop-words - histogram
	-s stop-words.out
	validate params and file
	read to device and create chunks,
	start & stop hash
explore read, fread, gets
check: do update_h_hist only before cross_sort.

Stop word implementation
in argument processing
	create_stop_word
	+- cuda_stop_word_read_buffer_into_gpu
	|	read stop word list
	|	mv to gpu.
	+- cuda_stream_to_token_start
	|	create histo_chunk (we are using histo to do it)
	|	copy_to_hash.  Do all the hash creation here
	|	K_histo_words_to_token_first_time
	+- cuda_create_stop_word_list
		do all the malloc & cudaMalloc in ab
		k_histo_hash_to_Sw_hash - mv hash to sw
		setup_sw_first_last: first and last in ab
		delete_histo_chunks
		reset the home chunk

in milib_gpu_sort_merge_histo_word - the stop word implementation is here
	sort_one_token_chunk - for each chunk sort each internally
	update_h_hist
	if stop_word is enabled
		for each chunk
			copy them to d_stp_wd_list
			apply_stop_words
			if stop_word is applied
				update_h_hist
			endif
		endfor
	sort_histo_chunks
	sort_merge_histo_cross

update_sw_ranges

delete_histo_val
if stopwd_first_last.first == stopwd_first_last.last
	delete histo
endif

if (loc == first)
	forward find loc of first data, inc it
	set loc to inc'd location
	del 1st data (host & dev)
else
	back find loc of first data, inc it
	set loc to inc'd location
	del 1st data (host & dev)
endif

sort
	THEN merge_sort
hash_mem is created in stop_words and will be 8k

We need to reduce the number of times d_data is copied to h_data
 we should bring down d_wds only.
 we are bringing d_data for h_data because of stop_word processing - relook at stop-word processing

stop-word.cu: What do we do about range values?
Intersection between histo & stop_words will get ~10% performance boost.  Low priority

Why is K_Histo_Math_Sort_Dlist_[even|odd] taking so many passes - 10 passes for 6 words?

d_stp_wd_IsDel should be set to false everytime new data is read into device.
math is not working
variance of math is not working properly.
check all the gTh.  I don't think we are doing gTh correctly. K_Histo_fill_stop_wd should incorporate d_nos...
do we need ab -> d_sw_first_last?
balance_from_histo is not merging completely

(we should move all zeros out quickly)
testing - in process_next_set_of_tokens
set first_time to false and test it

Should h_zero_swap not be the other way from curr -> next?
Yes - it is already prev to curr

should we not apply stop words before sorting
No - because stop words means 4k words per word and (assuming 30% of words are sw with duplicates) we save time by merging first.

should we not compress_zero before sorting
= Ans NO.  We are filling up the histo & any gaps are due to merging.  Assume 15-20% merging.  Not much bang for the buck & cost of getting data to & fro negates value.
Also, usually data will be right aligned and filled in most chunks except the first chunk.

Review all the comments - thare are tasks documented inside the code.

Make a list of TCs for all the functions and walk through the code to ensure they are tested properly.

pseudocode: Adding Status (NEW and PREVIOUS) for merging only modified data - primarily affects sorting

status -> NEW - impacts create_hist & reset_hist
sort only the new
PREVIOUS - after histo_sort.
Merge everything

h_new -> status = NEW (create_histo, reset_histo)
insert new_histo AFTER existing histo
track new_histos & existing histos

func: cuda_convert_to_words
	add data to new histo ONLY
func: milib_gpu_sort_merge_histo_word
	apply sort_one_token_chunk
	apply_stop_words
	sort_histo_chunks
	only to NEW histo

	after sort_histo_chunks set status = PREVIOUS

work on read_buffer_into_gpu - what if word is chopped
pseudocode: multiple read string that is broken across multiple lines
set everything up
if there is previous word
	copy prev word to buffer beginning & mv buf pointer to after the copy
	reduce the number of chars to be read by the chars copied.
else
	buf pointer is at buffer beginning
endif
read chars
if last char is a char or is -
	store the last set of chars as part of a word
	reduce buf read count by the chars stored
else
	strip off the blanks at the end.  This will include \n & \r
	reduce buf read count by the blanks stripped off
end

The second line should be added in front.
We should not sort the previously sorted chunks except when cross-sort-merge happens.

Do we need d_stp_wd_IsDel?
Yes we do.  This is used to track if there has been a stop-word delete happened.  Used in apply_stop_words and also in K_histo_apply_stop_words

we assume sort_histo_chunks is working properly
we haven't tested sort_histo_chunks properly - we have bypassed most of the code.

High level:
read multiple lines & merge into histo
testing & performance

inside the string if there is a \n or a \r\n separating two sub-words then we should remove these and join the words - check with Ravi Madhira

No need to check.  This is not possible.  A word will either end at . or at \n or \r\n or if it is broken into two then it is hyphenated.  The last char is a -.

A hyphen inside doesn't count.
max = 0 value

can we reword apply_stop_words such that we don't need update_h_hist before it? - Ans: No, not possible.  A lot of stop_word update happens here.  Needed for both stop_word and after sorting.

median = 1

write delete_word_chunk - delete_token_chunk only deletes the token, not the words
we need to update everything for low.

WE HAVE TO REDUCE THE NUMBER OF SWAPS
we need to swap from first half to 2nd half.
Also 1/4th 1/4th.

CHUNK_SIZE should be at least equal to the number of cpus on the card.
It should be divisible by 4
It should also be equal to the number of sorted/merged words + 20% - so that every subsequent merger results in just 1 chunk
It should also be proportional to max threads in a block
It should also be related to max_stop_words (ie. 595 words)

 Am I handling capitals properly - mHash will be different for capitals vs lower case.

implementing minimal strn_cpy
 read data in
 convert to words
 store words separately, sort using ptrs
 copy words only when sending data out.
cuda_milib.h
add list_of_words to all_buf
list_of_words is on host. Only histo_words is on device.
change token to have pointer to str
create list_of_words - str + len
mv len everywhere to list_of_words
(if str [0] == '\0' & len = 0 then no word)

create_buffer
	create list_of_words buffer in h & d
	(do we need h?)
process_next_set_of_tokens
	cuda_stream_to_wd_to_token
		after K_hist_copy_to_struct_for_hash_mem
		create h_hist_store_wds (update list_list_of_wds)
		as part of K_histo_wds_to_token
		copy words to list_of_words
		update ptr to d_list
Create CHUNK_SIZE of low.  All new words have new chunks of low.

Clean up create_low_for_token - should return low that can be added to h_new...
We need low_for_token for histo_balance too...  Do we?

define token_list, sorted token_tracking and h_tokens
modify code - esp. swap & merge

compile everything
remove all word_tracking
then change token_tracking to word_tracking
compile again.

gpu_calc - update across teh code

When we copy from gpu histo to host histo - are we copying right.  We are slimming down the gpu token vis histo_balance but are we doing the same thing for the host?

Getting data to and fro
h_read_buf should be pinned data and we should async memcpy to device. Also when we get the token back from device to host
wd_idx has to reduce as we merge words
check wd_idx is processed right & so is histo_idx.

when we bring d_data down to h_data, the h_wds ptr will be over writter.  Return the h_wds pointer.

Look at all CHUNK_SIZE and pass it as a parameter -may change with a new board

implementing minimal strn_cpy
debug and get it working...

don't do an internal sort.  First apply stop-words, then do balance & sort


===========================================================

TBD:===============  Just cp/paste from Notes   ==================
TBD:
High level:
read multiple lines & merge into histo
testing & performance
-------------------------
Do a code review.
The second line should be added in front.
We should not sort the previously sorted chunks except when cross-sort-merge happens.

Do we need d_stp_wd_IsDel?
-------------------------
we assume sort_histo_chunks is working properly
we haven't tested sort_histo_chunks properly - we have bypassed most of the code.
-------------------------
CHUNK_SIZE should be at least equal to the number of cpus on the card.

Postpone
 Am I handling capitals properly - mHash will be different for capitals vs lower case.

stop-word.cu: What do we do about range values?
Intersection between histo & stop_words will get ~10% performance boost.  Low priority

Why is K_Histo_Math_Sort_Dlist_[even|odd] taking so many passes - 10 passes for 6 words?

Can you reduce the number of strn_cpy.  Can we keep the string in one place and swap the address?
-------------------------
Getting data to and fro
h_read_buf should be pinned data and we should async memcpy to device. Also when we get the token back from device to host
wd_idx has to reduce as we merge words
check wd_idx is processed right & so is histo_idx.
-------------------------
All this is assuming we can fit the histogram inside the card.  Changes needed if the histogram is larger than the card.

sort
start 1/2 & 1/2 and keep reducing as sort goes through.  When sort happens, repeat till sort goes through till we get to switching next to each other.
then do an odd_even sort - one pass of big movement of data, then do odd-even quarter and then 1/8th and reducing

There is an issue in sorting.  If two sorted elements are next to each other and the third element is a zero. This sort happens in 3 passes.
1. 1st & secnd is sorted - nothing happens.
2 2nd and third is sorted - swap.
3 1st adn 2nd sorted - swap.

-------------------------
Multi-threaded design
main
 |
+- 3 threads: if *data_not_full
 |		read stream store in gpu & cpu
 |		update *data_not_full
 |
 +- 1 thread:  do histo and most of math
 |		(both cpu & gpu)
 |		if *data_not_full is not empty
 |			stream to words
 |			words to token
 |			token to histo
 | 			part math calc
 | 			update *is_histo
 |
 +- 1 thread: if *is_histo
 |		fetch histo_data to cpu
 |		merge cpu/gpu
 |		reduce *is_histo, set *histo_fetched
 |
 +- 1 thread: if *histo_fetched
 		do the math
 		update *histo_fetched
